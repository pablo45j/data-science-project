{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RodolfoFerro/dl-facilito-g2/blob/main/notebooks/Deep_Learning_Clase_4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R6jO_1gISKxk"
      },
      "source": [
        "# Drowsiness Detection\n",
        "\n",
        "> **Descripción:** Este proyecto consiste en el entrenamiento de una red neuronal para determinar la somnolencia de conductos a través de la detección de los ojos cerrados.<br>\n",
        "> **Autor:** [Pablo Juárez del Valle] <br>\n",
        "> **Contacto:** [Gmail](pablo.juarezdelvalle@gmail.com) \n",
        "\n",
        "\n",
        "## Contenido\n",
        "\n",
        "### Preámbulo\n",
        "\n",
        "- Reconocimiento de imagenes\n",
        "\n",
        "### Definición del problema\n",
        "\n",
        "La Organización Mundial de la Salud (OMS) estima que anualmente 1,35 millones de personas en el mundo resultan víctimas mortales por lesiones del tránsito y entre 20 y 50 millones padecen secuelas físicas y psicológicas, siendo esta una de las principales causas de discapacidad [1]. En Argentina, considerando el año 2019 por ser el último anterior a la aparición de la pandemia a causa del COVID-19 (hecho que modificó los datos de movilidad a nivel nacional), la Agencia Nacional de Seguridad Vial (ANSV) registró 99.221 siniestros viales con víctimas [2], los cuales dejaron como consecuencia 4.911 personas fallecidas a causa de la inseguridad vial en Argentina.\n",
        "\n",
        "El estrés es la reacción del cuerpo a un desafío o demanda que puede provenir de cualquier situación o pensamiento de furia, ansiedad, nervios o frustración. En cambio, la fatiga, si bien coloquialmente es sinónimo de cansancio y ambas palabras se utilizan indistintamente, se encuentra acompañada de otros síntomas generales como dolor generalizado, ansiedad, depresión, apatía, alteraciones del sueño, o alteraciones de la memoria o de la concentración; mientras que el cansancio se origina como consecuencia de una actividad física [3].\n",
        "\n",
        "El estrés y la fatiga son factores de riesgo que, en general, son difíciles de medir y ser admitidos por las/os parecientes; mucho más cuando se presentan en rubros laborales donde la conducción de vehículos es la tarea principal y la prolongación del horario tras el volante reditúa económicamente. Conducir es una tarea compleja que involucra aspectos como la percepción, el tiempo de respuesta y la capacidad física, haciendo de la coordinación moto-sensorial un punto fundamental para realizar maniobras que requieren de varios movimientos simultáneos y de manera coordinada. En tal sentido, y en línea con la literatura especializada, se entiende a la conducción de vehículos bajo fatiga y/o estrés como uno de los principales factores de riesgo de la siniestralidad vial [4].\n",
        "\n",
        "### Objetivos\n",
        "\n",
        "El objetivo del proyecto consiste en diseñar una ANN convolucional y entrenarla para la detección de somnolencia mediante la obtención de fotos. Los datos de entrenamiento son obtenidos de Kaggle. [5]\n",
        "1. Se implementará una red convolucional siguiendo el modelo LeNet5 de Yann LeCun para su entrenamiento y posterior predicción.\n",
        "2. Se desarrollará una API para cargar fotos de ojos y detectar la somnolencia.\n",
        "3. Sentar las bases del prototipo para implementarlo en un dispositivo que pueda instalarse en un vehiculo y mediante la toma de fotos pueda detectar si el conductor tiene sueño.\n",
        "\n",
        "#### Referencias\n",
        "\n",
        "1. <a href=\"www.who.int/publications/i/item/9789241565684\">OMS (2018): Global Status Report on Road Safety.</a>\n",
        "2. <a href=\"www.argentina.gob.ar/sites/default/files/2018/12/ansv_ov_anuario_estadistico_2019_final.pdf\">Anuario nuario Estadístico de la Seguridad Vial 2019 de la ANSV.</a>\n",
        "3. <a href=\"https://www.infosalus.com/salud-investigacion/noticia-fatiga-cuando-debemos-sospechar-algo-no-va-bien-20210720083435.html\">Publicación del 20-7-2021 en el sitio Infosalus.com, editado por Europapress, España.</a>\n",
        "4. <a href=\"www.argentina.gob.ar/sites/default/files/ansv_observatorio_situacion_seguridad_vial_arg.pdf\">ANSV (2018): Situación de la seguridad vial en Argentina.</a> \n",
        "5. <a href=\"https://www.kaggle.com/datasets/prasadvpatil/mrl-dataset\">Detección de somnolencia en conductores</a>\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xNVG2PnSEtQN"
      },
      "source": [
        "## **Preámbulo**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IOOWWpUv32iz"
      },
      "source": [
        "### Redes Convolucionales\n",
        "\n",
        "\n",
        "Una red neuronal convolucional es un tipo de red neuronal artificial donde las neuronas artificiales, corresponden a campos receptivos de una manera muy similar a las neuronas en la corteza visual primaria de un cerebro biológico, es decir distintas capas. Donde cada capa en la que el modelo convoluciona, aplica un filtro por ejemplo, aportando información.\n",
        "Este tipo de redes son muy efectivas para tareas de visión artificial, como en la clasificación y segmentación de imágenes, entre otras aplicaciones.​ \n",
        "\n",
        "Un modelo de red neuronal convolucional profunda, muy difundido por el investigador Yann LeCun, es el modelo LeNet5.\n",
        "\n",
        "<center>\n",
        "    <img src=\"https://www.datasciencecentral.com/wp-content/uploads/2021/10/1lvvWF48t7cyRWqct13eU0w.jpeg\" width=\"60%\">\n",
        "</center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Código"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1kb3YuuW51-b"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras.datasets import fashion_mnist\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras import layers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1RbMybth8xL5"
      },
      "source": [
        "Cargamos los datos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "58k-OiKs7WQW"
      },
      "outputs": [],
      "source": [
        "(x_train, _), (x_test, _) = fashion_mnist.load_data()\n",
        "\n",
        "x_train = x_train.astype('float32') / 255.\n",
        "x_test = x_test.astype('float32') / 255.\n",
        "\n",
        "print (x_train.shape)\n",
        "print (x_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hr5CjhCK-Asn"
      },
      "outputs": [],
      "source": [
        "plt.imshow(x_train[0], cmap='gray')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_9VwrJXJ8YSy"
      },
      "source": [
        "Creamos la clase Autoencoder con TensorFlow."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1qeFNIHw7i4u"
      },
      "outputs": [],
      "source": [
        "class Autoencoder(Model):\n",
        "    def __init__(self, latent_dim):\n",
        "        super(Autoencoder, self).__init__()\n",
        "        self.latent_dim = latent_dim   \n",
        "\n",
        "        # Construimos el encoder\n",
        "        self.encoder = tf.keras.Sequential([\n",
        "\n",
        "            # TODO: Añade una capa Flatten\n",
        "            \n",
        "            # TODO: Añade una capa Dense -> latent_dim, ReLU\n",
        "\n",
        "        ])\n",
        "\n",
        "        # Construimos el decoder\n",
        "        self.decoder = tf.keras.Sequential([\n",
        "            \n",
        "            # TODO: Añade una capa Dense -> 784, Sigmoid\n",
        "            \n",
        "            # TODO: Añade una capa Reshape -> (28, 28)\n",
        "\n",
        "        ])\n",
        "\n",
        "    def call(self, x):\n",
        "        # Esta función nos permite invocar los sub modelos\n",
        "        # para poder hacer inferencia sobre cada uno (predict)\n",
        "        encoded = self.encoder(x)\n",
        "        decoded = self.decoder(encoded)\n",
        "        \n",
        "        return decoded"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kx5QY8uY8qSj"
      },
      "source": [
        "Definimos parámetros y creamos un autoencoder."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZO8FSy2S7pXj"
      },
      "outputs": [],
      "source": [
        "# Definimos dimensión de espacio latente\n",
        "latent_dim = 64\n",
        "\n",
        "# Instanciamos un autoencoder\n",
        "autoencoder = Autoencoder(latent_dim)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GjfNJhov8tYX"
      },
      "source": [
        "Compilamos y entrenamos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Si_2tc2u8is5"
      },
      "outputs": [],
      "source": [
        "autoencoder.compile(optimizer='adam', loss='mse')\n",
        "\n",
        "history = autoencoder.fit(x_train, x_train,\n",
        "                          epochs=10,\n",
        "                          shuffle=True,\n",
        "                          validation_data=(x_test, x_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mZexUsya-dwM"
      },
      "outputs": [],
      "source": [
        "autoencoder.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YWm7Pi2Q-fdx"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.style.use('seaborn-v0_8')\n",
        "\n",
        "x = np.arange(len(history.history['loss']))\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(x, loss, label='Training')\n",
        "plt.plot(x, val_loss, label='Validation')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.savefig('loss_autoencoder.png', dpi=300)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sUdWJBEO8vC6"
      },
      "source": [
        "Exploramos los resultados."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y8ZfJ8J__j5f"
      },
      "outputs": [],
      "source": [
        "encoded_imgs = autoencoder.encoder(x_test).numpy()\n",
        "decoded_imgs = autoencoder.decoder(encoded_imgs).numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DF9NFmQ-7q_l"
      },
      "outputs": [],
      "source": [
        "n = 10\n",
        "plt.figure(figsize=(20, 4))\n",
        "\n",
        "for i in range(n):\n",
        "    # Mostramos imagen original\n",
        "    ax = plt.subplot(2, n, i + 1)\n",
        "    plt.imshow(x_test[i + 121])\n",
        "    plt.title(\"Original\")\n",
        "    plt.gray()\n",
        "    ax.get_xaxis().set_visible(False)\n",
        "    ax.get_yaxis().set_visible(False)\n",
        "\n",
        "    # Mostramos imagen reconstruida\n",
        "    ax = plt.subplot(2, n, i + 1 + n)\n",
        "    plt.imshow(decoded_imgs[i + 121])\n",
        "    plt.title(\"Reconstruida\")\n",
        "    plt.gray()\n",
        "    ax.get_xaxis().set_visible(False)\n",
        "    ax.get_yaxis().set_visible(False)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7r1Tx5AyBAK3"
      },
      "source": [
        "**Reto:** ¿Puedes mejorar aún más las reconstrucciones?\n",
        "\n",
        "Te recomiendo explorar lo siguiente:\n",
        "- Modifica el número de capas y neuronas por capa. Recuerda que debe ir disminuyendo en el encoder y aumentando en el decoder y deben tener una estructura espejeada.\n",
        "- Modifica el número de épocas de entrenamiento.\n",
        "- ¿Quieres intentar reconstruir cosas con convoluciones? Te invito a que lo intentes.\n",
        "\n",
        "\n",
        "**Lecturas recomendadas:**\n",
        "- [A 2021 Guide to improving CNNs-Optimizers: Adam vs SGD](https://medium.com/geekculture/a-2021-guide-to-improving-cnns-optimizers-adam-vs-sgd-495848ac6008)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rtpuj0PBqWYc"
      },
      "source": [
        "## **Sección XI**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jJdHo17RAp4S"
      },
      "source": [
        "\n",
        "### Limitaciones de autoencoders básicos\n",
        "\n",
        "- **Incapacidad para capturar información espacial:** Los autoencoders básicos pueden tener dificultades para capturar la estructura espacial de las imágenes, ya que no tienen en cuenta la información de vecindad de los píxeles.\n",
        "- **Sensibilidad a las transformaciones:** Pueden ser sensibles a las transformaciones geométricas, como la rotación o el desplazamiento de la imagen, lo que puede afectar su capacidad de reconstrucción.\n",
        "- **Autoencoders convolucionales como solución:** Son una variante de los autoencoders que incorporan capas convolucionales para abordar las limitaciones mencionadas.\n",
        "- **Ventajas de los autoencoders convolucionales:** Mejoran con la capacidad para capturar algunas características espaciales, preservar la estructura y ser más robustos frente a transformaciones geométricas\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vs8JZj5D6Jgx"
      },
      "source": [
        "### Autoencoders convolucionales\n",
        "\n",
        "La estructura de un autoencoder convolucional consta de dos partes principales: el codificador (encoder) y el decodificador (decoder). Cada una de estas partes está compuesta por capas convolucionales, capas de muestreo y, en algunos casos, capas de convolución transpuesta o de upsampling. \n",
        "\n",
        "<center>\n",
        "    <img src=\"https://miro.medium.com/v2/resize:fit:4800/format:webp/1*TOJD69Y8dZsKFEW-21xUPg.png\" width=\"70%\">\n",
        "</center>\n",
        "\n",
        "#### Encoder\n",
        "\n",
        "- **Capas convolucionales:** Estas capas utilizan filtros convolucionales para extraer características de nivel superior de la entrada.\n",
        "- **Capas de pooling:** Estas capas reducen la dimensionalidad de las características extraídas al realizar un muestreo o reducción de tamaño, como el max pooling.\n",
        "- **Funciones de activación:** Después de cada capa convolucional y de muestreo, se aplica una función de activación no lineal, como la función ReLU (Rectified Linear Unit), para introducir no linealidad en la red.\n",
        "\n",
        "#### Espacio latente\n",
        "\n",
        "- **Capa de aplanamiento:** Antes de llegar al espacio latente, la salida del codificador se aplanará en un vector unidimensional.\n",
        "- **Capa densa (fully connected):** La capa densa o fully connected reduce aún más la dimensionalidad y mapea las características a un espacio latente de menor dimensión. Esta capa suele tener una función de activación, como la ReLU o la tangente hiperbólica.\n",
        "\n",
        "#### Decoder\n",
        "\n",
        "- **Capas densas (fully connected):** En el decodificador, se utilizan capas densas para aumentar gradualmente la dimensionalidad del espacio latente y reconstruir las características originales.\n",
        "- **Capas de convolución transpuesta o upsampling:** Estas capas realizan la operación inversa de las capas de muestreo, aumentando gradualmente el tamaño espacial de las características.\n",
        "- **Capas de convolución:** Al final del decodificador, se utilizan capas convolucionales para generar una salida final con las mismas dimensiones que la entrada original.\n",
        "- **Función de activación final:** La función de activación final depende del rango de valores de la imagen de salida. Por ejemplo, en imágenes en escala de grises, se puede utilizar una función de activación sigmoide para obtener valores entre 0 y 1.\n",
        "\n",
        "La estructura del autoencoder convolucional puede variar según la tarea específica y los requisitos del problema. Se pueden agregar capas adicionales, como capas de regularización, capas de normalización o capas de convolución dilatadas, para mejorar el rendimiento y la capacidad de generalización del modelo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8KKqdPJBCU_E"
      },
      "source": [
        "## **Sección XII**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WCO2eFvsCOVs"
      },
      "source": [
        "\n",
        "### Implementación y entrenamiento de los autoencoders convolucionales"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "stCt0xNzzk_V"
      },
      "outputs": [],
      "source": [
        "class Denoise(Model):\n",
        "    def __init__(self):\n",
        "        super(Denoise, self).__init__()\n",
        "\n",
        "        # Creamos el encoder convolucional\n",
        "        self.encoder = tf.keras.Sequential([\n",
        "\n",
        "            # TODO: Añade una capa Input -> shape=(28, 28, 1)\n",
        "            \n",
        "            # TODO: Añade una capa Conv2D -> 16, (3, 3), activation='relu', padding='same', strides=2\n",
        "            \n",
        "            # TODO: Añade una capa Conv2D -> 8, (3, 3), activation='relu', padding='same', strides=2\n",
        "        \n",
        "        ])\n",
        "\n",
        "        # Creamos el decoder convolucional\n",
        "        self.decoder = tf.keras.Sequential([\n",
        "\n",
        "            # TODO: Añade una capa Conv2DTranspose -> 8, kernel_size=3, activation='relu', padding='same', strides=2\n",
        "            \n",
        "            # TODO: Añade una capa Conv2DTranspose -> 16, kernel_size=3, activation='relu', padding='same', strides=2\n",
        "            \n",
        "            # TODO: Añade una capa Conv2D -> 1, (3, 3), activation='sigmoid', padding='same'\n",
        "        \n",
        "        ])\n",
        "\n",
        "    def call(self, x):\n",
        "        # Esta función nos permite invocar los sub modelos\n",
        "        # para poder hacer inferencia sobre cada uno (predict)\n",
        "        encoded = self.encoder(x)\n",
        "        decoded = self.decoder(encoded)\n",
        "        \n",
        "        return decoded"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l2OVuOuwHqv0"
      },
      "source": [
        "**Lecturas recomendadas:**\n",
        "\n",
        "- [Convolution, Padding, Stride, and Pooling in CNN](https://medium.com/analytics-vidhya/convolution-padding-stride-and-pooling-in-cnn-13dc1f3ada26)\n",
        "- [Why do we need conv2d_transpose?\n",
        "](https://medium.com/@vaibhavshukla182/why-do-we-need-conv2d-transpose-2534cd2a4d98)\n",
        "- [Deconvolution](https://vincmazet.github.io/bip/restoration/deconvolution.html)\n",
        "- [Image Segmentation using deconvolution layer in Tensorflow](https://cv-tricks.com/image-segmentation/transpose-convolution-in-tensorflow/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aMBsYpdK6cIN"
      },
      "source": [
        "### Aplicaciones de los autoencoders convolucionales en el denoising de imágenes\n",
        "\n",
        "Para poder explorar un ejemplo de aplicación de autoencoders donde los utilicemos para quitar el ruido de alunas imágenes, volveremos a utilizar el dataset de modas y le agregaremos algo de ruido a las imágenes. De este modo, entrenaremos a la red para que aprenda cómo debe verse una imagen a partir de una con ruido.\n",
        "\n",
        "Cargaremos y preprocesaremos nuevamente los datos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "erEOQdn0C5x1"
      },
      "outputs": [],
      "source": [
        "(x_train, _), (x_test, _) = fashion_mnist.load_data()\n",
        "\n",
        "x_train = x_train.astype('float32') / 255.\n",
        "x_test = x_test.astype('float32') / 255.\n",
        "\n",
        "x_train = x_train[..., tf.newaxis]\n",
        "x_test = x_test[..., tf.newaxis]\n",
        "\n",
        "print(x_train.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yepxPRZ0DEPB"
      },
      "source": [
        "Agregamos ruido a los datos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NyGaNXfnDDfc"
      },
      "outputs": [],
      "source": [
        "noise_factor = 0.2\n",
        "x_train_noisy = x_train + noise_factor * tf.random.normal(shape=x_train.shape) \n",
        "x_test_noisy = x_test + noise_factor * tf.random.normal(shape=x_test.shape) \n",
        "\n",
        "x_train_noisy = tf.clip_by_value(x_train_noisy, clip_value_min=0., clip_value_max=1.)\n",
        "x_test_noisy = tf.clip_by_value(x_test_noisy, clip_value_min=0., clip_value_max=1.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IaWoRWDsDkou"
      },
      "source": [
        "Podemos explorar cómo se ven los datos con algo de ruido."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "31WxIj_jDkQ8"
      },
      "outputs": [],
      "source": [
        "n = 10\n",
        "plt.figure(figsize=(20, 2))\n",
        "for i in range(n):\n",
        "    ax = plt.subplot(1, n, i + 1)\n",
        "    plt.title(\"Original + Ruido\")\n",
        "    plt.imshow(tf.squeeze(x_test_noisy[i]))\n",
        "    plt.gray()\n",
        "    ax.set_xticks([])\n",
        "    ax.set_yticks([])\n",
        "\n",
        "plt.grid(False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vziHzY1CDc7U"
      },
      "source": [
        "Instanciaremos, compilaremos y entrenaremos un nuevo autoencoder llamado \"denoiser\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "quxg9yeHAdR4"
      },
      "outputs": [],
      "source": [
        "denoiser = Denoise()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "38IRvJY17Ivb"
      },
      "outputs": [],
      "source": [
        "denoiser.compile(optimizer='adam', loss='mse')\n",
        "\n",
        "history = denoiser.fit(x_train_noisy, x_train,\n",
        "                       epochs=10,\n",
        "                       shuffle=True,\n",
        "                       validation_data=(x_test_noisy, x_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oMRtH2wTFMPk"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.style.use('seaborn-v0_8')\n",
        "\n",
        "x = np.arange(len(history.history['loss']))\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(x, loss, label='Training')\n",
        "plt.plot(x, val_loss, label='Validation')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.savefig('loss_denoiser.png', dpi=300)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "USi-p3n5Dsvu"
      },
      "source": [
        "Podemos explorar los detalles de ambos submodelos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E8UXlSb0DiBk"
      },
      "outputs": [],
      "source": [
        "denoiser.encoder.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5LKiHWpSDv_E"
      },
      "outputs": [],
      "source": [
        "denoiser.decoder.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KRJUL90BD0Qx"
      },
      "source": [
        "Aplicamos la inferencia sobre las imágenes ruidosas utilizando ambos modelos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GzQ4GoZFDxr_"
      },
      "outputs": [],
      "source": [
        "encoded_imgs = denoiser.encoder(x_test_noisy).numpy()\n",
        "decoded_imgs = denoiser.decoder(encoded_imgs).numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SdIqgkt1Dzvc"
      },
      "outputs": [],
      "source": [
        "n = 10\n",
        "plt.figure(figsize=(20, 6))\n",
        "\n",
        "\n",
        "for i in range(n):\n",
        "\n",
        "    # Mostramos original + ruido\n",
        "    ax = plt.subplot(3, n, i + 1)\n",
        "    plt.title(\"Original + Ruido\")\n",
        "    plt.imshow(tf.squeeze(x_test_noisy[i + 121]))\n",
        "    plt.gray()\n",
        "    ax.get_xaxis().set_visible(False)\n",
        "    ax.get_yaxis().set_visible(False)\n",
        "\n",
        "    # Mostramos reconstrucción\n",
        "    bx = plt.subplot(3, n, i + n + 1)\n",
        "    plt.title(\"Reconstruida\")\n",
        "    plt.imshow(tf.squeeze(decoded_imgs[i + 121]))\n",
        "    plt.gray()\n",
        "    bx.get_xaxis().set_visible(False)\n",
        "    bx.get_yaxis().set_visible(False)\n",
        "\n",
        "    # Mostramos original\n",
        "    cx = plt.subplot(3, n, i + n + n + 1)\n",
        "    plt.title(\"Original\")\n",
        "    plt.imshow(tf.squeeze(x_test[i + 121]))\n",
        "    plt.gray()\n",
        "    cx.get_xaxis().set_visible(False)\n",
        "    cx.get_yaxis().set_visible(False)\n",
        "\n",
        "plt.savefig('imagen_sin_ruido.png', dpi=300)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8U1tdrMYlDen"
      },
      "source": [
        "**Reto:** ¿Puedes mejorar aún más el modelo?\n",
        "\n",
        "Te recomiendo explorar lo siguiente:\n",
        "- Modifica el número de capas y parámetros de convolución por capa.\n",
        "- Modifica el número de épocas de entrenamiento.\n",
        "- Puedes explorar el remover ruido de imágenes con más canales (imágenes RGB) utilizando otros datasets."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JP3NGkSg6fO2"
      },
      "source": [
        "### Trabajos relacionados y avances recientes\n",
        "\n",
        "\n",
        "Ha habido varios trabajos de investigación y avances recientes que han contribuido al desarrollo de nuevas arquitecturas, técnicas de entrenamiento mejoradas y aplicaciones emergentes.\n",
        "\n",
        "- **UNet:** Es ampliamente utilizada en el campo de la segmentación de imágenes, pero también se ha aplicado con éxito en tareas de denoising.\n",
        "- **Variational Autoencoders (VAEs):** Los VAEs son una variante de los autoencoders que se utilizan para el aprendizaje de distribuciones latentes. Han demostrado ser efectivos en el denoising de imágenes al aprender representaciones latentes que siguen una distribución probabilística, lo que permite una generación más controlada y realista de imágenes limpias.\n",
        "- **GANs y Autoencoders Generativos (GANs-AE):** La combinación de las GANs y los AE ha llevado al desarrollo de los GANs-AE. Estos modelos aprovechan la capacidad de los GANs para generar imágenes realistas y los autoencoders para aprender representaciones latentes eficientes. Los GANs-AE han demostrado ser efectivos en el denoising y la generación de imágenes de alta calidad.\n",
        "\n",
        "\n",
        "#### **Tareas en el campo de la visión artificial**\n",
        "\n",
        "1. **Clasificación de imágenes:** La tarea de clasificación de imágenes implica asignar una etiqueta o categoría a una imagen de entrada. Esto implica entrenar un modelo para reconocer y distinguir diferentes objetos, personas o escenas en una imagen.\n",
        "2. **Detección de objetos:** La detección de objetos implica localizar y clasificar múltiples objetos en una imagen. El objetivo es detectar la presencia y la ubicación de objetos específicos en una escena, a menudo utilizando cuadros delimitadores para delinear las regiones donde se encuentran los objetos.\n",
        "3. **_Denoising_ o reconstrucción de imágenes:** Consiste en eliminar o reducir el ruido presente en una imagen, obteniendo una versión más limpia y clara. Esta tarea es relevante en áreas como la fotografía, la medicina y la seguridad..\n",
        "4. **Segmentación semántica:** La segmentación semántica implica asignar una etiqueta a cada píxel de una imagen para identificar y delimitar las diferentes regiones o objetos presentes. El objetivo es comprender la estructura y el contenido de una imagen a nivel de píxel.\n",
        "5. **Detección de rostros:** La detección de rostros es una tarea específica de la visión artificial que implica detectar y localizar los rostros en una imagen. Es ampliamente utilizado en aplicaciones de reconocimiento facial, análisis de emociones y sistemas de seguridad.\n",
        "6. **Reconocimiento y verificación facial:** El reconocimiento facial se refiere a la tarea de identificar y reconocer a una persona específica a partir de una imagen o secuencia de imágenes. La verificación facial se enfoca en verificar si una imagen de rostro coincide con una identidad específica.\n",
        "7. **Estimación de pose:** La estimación de pose se refiere a la tarea de determinar la posición y orientación de un objeto o persona en una imagen. Esto implica detectar y rastrear las articulaciones o puntos clave en una imagen para comprender la postura y el movimiento.\n",
        "8. **Estimación de profundidad:** La estimación de profundidad implica inferir la información de la distancia o la profundidad de los objetos en una imagen. Es útil en aplicaciones de realidad virtual, conducción autónoma y sistemas de navegación.\n",
        "9. **Super-resolución:** La super-resolución se refiere a aumentar la resolución o la calidad de una imagen de baja resolución. El objetivo es generar una versión de alta resolución que capture más detalles y claridad.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hSdbQU3e6-Ky"
      },
      "source": [
        "--------\n",
        "\n",
        "> Contenido creado por **Rodolfo Ferro**, 2023. <br>\n",
        "> Puedes contactarme a través de Insta ([@rodo_ferro](https://www.instagram.com/rodo_ferro/)) o Twitter ([@rodo_ferro](https://twitter.com/rodo_ferro))."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.2 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.2"
    },
    "vscode": {
      "interpreter": {
        "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
